{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Logistic Regression (Solutions)\n",
        "<sub><sup>*written by riya karumanchi & isabel sieh, cs124 staff team, winter '25/'26*</sup></sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Sigmoid Intuition\n",
        "\n",
        "First, form a group of 3 students to work together! Introduce yourselves to one another.\n",
        "\n",
        "In logistic regression, we compute a real-valued score\n",
        "$$\n",
        "z = w\\cdot x + b\n",
        "$$\n",
        "and then turn it into a probability using the sigmoid (logistic) function\n",
        "$$\n",
        "\\hat{y} = s(z)=\\frac{1}{1+\\exp(-z)}.\n",
        "$$\n",
        "**$z$** can be any real number, but **$s(z)$** is always between 0 and 1, and it \"squashes\" large-magnitude values toward 0 or 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A. \"Same sign, different confidence\"\n",
        "\n",
        "This part is meant to be straightforward—it's a sanity check to make sure you can interpret the sigmoid curve correctly. Look at the sigmoid function plotted below:\n",
        "\n",
        "![Sigmoid function](sigmoid.png)\n",
        "\n",
        "Below are three different scores ($z$). Using the figure above, rank the predicted probabilities ($\\hat{y}=s(z)$) from largest to smallest, and briefly justify using only the *shape* of the sigmoid:\n",
        "\n",
        "1. $z = 0.2$\n",
        "2. $z = 2$\n",
        "3. $z = 6$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 1.** Which one has the highest probability of $y=1$? Which one has the lowest? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rank the probabilities:\n",
        "$$\n",
        "s(6) > s(2) > s(0.2).\n",
        "$$\n",
        "- Highest probability of $y=1$: $z=6$\n",
        "- Lowest probability of $y=1$: $z=0.2$\n",
        "\n",
        "Justification: sigmoid is increasing; larger $z$ corresponds to larger $s(z)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 2.** Which pair is *closer together* as probabilities: $s(0.2)$ vs $s(2)$, or $s(2)$ vs $s(6)$? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which pair is closer together as probabilities?\n",
        "$$\n",
        "s(2)\\ \\text{vs}\\ s(6)\\ \\text{are closer}.\n",
        "$$\n",
        "Justification: the sigmoid curve is **flatter for large positive $z$** (saturation), so increasing $z$ from 2 to 6 changes probability less than increasing $z$ from 0.2 to 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B. \"How the sigmoid moves points\"\n",
        "\n",
        "Given reference values:\n",
        "* $s(0)=0.5$\n",
        "* $s(2)\\approx 0.88$\n",
        "* $s(-2)\\approx 0.12$\n",
        "* $s(4)\\approx 0.98$\n",
        "* $s(-4)\\approx 0.02$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 3.** Suppose two different feature vectors produce scores $z=2$ and $z=4$. In terms of probability, how much did the prediction change? What does this illustrate about \"squashing\"? (One sentence.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare $z=2$ and $z=4$.\n",
        "\n",
        "From the table:\n",
        "- $s(2)\\approx 0.88$\n",
        "- $s(4)\\approx 0.98$\n",
        "\n",
        "So:\n",
        "$$\n",
        "\\Delta z = +2,\\quad \\Delta \\hat{y} \\approx 0.98 - 0.88 = +0.10.\n",
        "$$\n",
        "\n",
        "**Key point:** Even though the score $z$ increased by $+2$, the probability only increased by about $+0.10$, illustrating that sigmoid \"squashes\" large logits, so probabilities show diminishing returns near 0/1 (saturation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 4.** Now compare $z=-2$ and $z=-4$. What symmetry do you notice?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare $z=-2$ and $z=-4$. What symmetry do you notice?\n",
        "\n",
        "From the table:\n",
        "- $s(-2)\\approx 0.12$\n",
        "- $s(-4)\\approx 0.02$\n",
        "\n",
        "Symmetry property:\n",
        "$$\n",
        "s(-z)=1-s(z).\n",
        "$$\n",
        "Interpretation: logits of equal magnitude but opposite sign produce probabilities that are \"mirrors\" around 0.5 (e.g. 0.88 ↔ 0.12, 0.98 ↔ 0.02)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now go back to the whole class and discuss group answers for Part 1 in a plenary session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: One step of gradient descent (how logistic regression learns)\n",
        "\n",
        "For the following problem, please choose a group facilitator/representative who will also take notes on your discussion.\n",
        "\n",
        "Each document (sentence/comment) is converted into a feature vector ($x$). The model computes:\n",
        "\n",
        "* **score (logit):** $z = w\\cdot x + b$\n",
        "\n",
        "  * $x$ is the feature vector for the document\n",
        "  * $w$ is the weight vector (one weight per feature)\n",
        "  * $b$ is a bias term (a constant offset)\n",
        "\n",
        "* **predicted probability:** $\\hat{y} = s(z)$, where\n",
        "  $$\n",
        "  s(z)=\\frac{1}{1+\\exp(-z)}\n",
        "  $$\n",
        "  $\\hat{y}$ is the model's predicted probability that $y=1$ for this document.\n",
        "\n",
        "* **true label:** $y \\in \\{0,1\\}$ is the correct label for the document\n",
        "\n",
        "  * $y=1$: positive (or \"class 1\")\n",
        "  * $y=0$: negative (or \"class 0\")\n",
        "\n",
        "The loss we use in lecture is the cross-entropy loss ($L_{CE}$). For a single training example ($(x,y)$), the derivative of the loss with respect to weight $w_j$ is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_{CE}}{\\partial w_j} = [s(w\\cdot x + b)-y]x_j\n",
        "$$\n",
        "\n",
        "Here is what each term means, in plain language:\n",
        "\n",
        "* $L_{CE}$: the loss (how \"wrong\" the model is on this example)\n",
        "* $w_j$: the weight for feature $j$\n",
        "* $x_j$: the value of feature $j$ for this document\n",
        "* $w\\cdot x + b$: the score ($z$) (total evidence before applying sigmoid)\n",
        "* $s(w\\cdot x + b)$: the predicted probability ($\\hat{y}$)\n",
        "* $\\hat{y}-y$: the \"error term\" (positive if we predicted too high; negative if we predicted too low)\n",
        "\n",
        "Finally, gradient descent updates parameters by moving **against** the gradient. \n",
        "\n",
        "$$\n",
        "q \\leftarrow q - h g\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $q$ is the parameter vector (it contains the weights and bias)\n",
        "* $h$ is the learning rate\n",
        "* $g$ is the gradient vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n",
        "\n",
        "We will classify a **movie review comment** using two word-count features:\n",
        "\n",
        "* $x_1$ = number of **positive** words in the comment (from a small positive lexicon)\n",
        "* $x_2$ = number of **negative** words in the comment (from a small negative lexicon)\n",
        "\n",
        "Consider the comment:\n",
        "\n",
        "> \"The acting was great and the soundtrack was incredible, and the cinematography was amazing — but the plot was boring and the ending was bad.\"\n",
        "\n",
        "Assume our lexicons contain:\n",
        "\n",
        "* positive words: {great, incredible, amazing}\n",
        "* negative words: {boring, bad}\n",
        "\n",
        "So the feature vector is:\n",
        "\n",
        "* $x_1 = 3$ (great, incredible, amazing)\n",
        "* $x_2 = 2$ (boring, bad)\n",
        "* $x = [3,2]$\n",
        "\n",
        "We will start with:\n",
        "\n",
        "* $w_1 = 0$, $w_2 = 0$, $b = 0$\n",
        "\n",
        "So initially:\n",
        "$$\n",
        "z = w\\cdot x + b = 0 \\quad\\Rightarrow\\quad \\hat{y}=s(0)=0.5\n",
        "$$\n",
        "\n",
        "Let the learning rate be $h = 0.1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Case 1: The comment is labeled positive ($y=1$)\n",
        "\n",
        "1. Compute $\\hat{y}-y$ at initialization. Is it positive or negative?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute $\\hat{y}-y$:\n",
        "$$\n",
        "\\hat{y}-y = 0.5 - 1 = -0.5\n",
        "$$\n",
        "Negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Using\n",
        "   $$\n",
        "   \\frac{\\partial L_{CE}}{\\partial w_j} = (\\hat{y}-y)x_j,\n",
        "   $$\n",
        "   determine the sign (positive or negative) of:\n",
        "\n",
        "   * $\\frac{\\partial L_{CE}}{\\partial w_1}$\n",
        "   * $\\frac{\\partial L_{CE}}{\\partial w_2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Signs of gradients:\n",
        "$$\n",
        "\\frac{\\partial L_{CE}}{\\partial w_1} = (\\hat{y}-y)x_1 = (-0.5)(3) < 0\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L_{CE}}{\\partial w_2} = (\\hat{y}-y)x_2 = (-0.5)(2) < 0\n",
        "$$\n",
        "So both gradient components are negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Gradient descent updates:\n",
        "   $$\n",
        "   w_j \\leftarrow w_j - h\\frac{\\partial L_{CE}}{\\partial w_j}\n",
        "   $$\n",
        "   Will $w_1$ increase or decrease? Will $w_2$ increase or decrease?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Update subtracts the gradient, so subtracting a negative value increases each weight:\n",
        "- $w_1$ increases\n",
        "- $w_2$ increases\n",
        "\n",
        "(Optional numeric update:)\n",
        "$$\n",
        "w_1 \\leftarrow 0 - 0.1(-1.5)= +0.15,\\quad\n",
        "w_2 \\leftarrow 0 - 0.1(-1.0)= +0.10.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. After this update, will the new score $z = w\\cdot x + b$ be larger or smaller than before?\n",
        "   Therefore, will $\\hat{y}=s(z)$ move toward **1** or toward **0**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since $x_1,x_2>0$ and both weights increased, the score $z=w\\cdot x+b$ increases. Therefore $\\hat{y}=s(z)$ increases and moves toward 1.\n",
        "\n",
        "(Optional numeric check:)\n",
        "$$\n",
        "z_{\\text{new}} = (0.15)(3) + (0.10)(2) + 0 = 0.65 \\Rightarrow \\hat{y}_{\\text{new}} = s(0.65) > 0.5.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Case 2: The same comment is labeled negative ($y=0$)\n",
        "\n",
        "Repeat Questions 1–4, but with $y=0$.\n",
        "\n",
        "5. What is the sign of $\\hat{y}-y$ now?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute $\\hat{y}-y$:\n",
        "$$\n",
        "\\hat{y}-y = 0.5 - 0 = +0.5\n",
        "$$\n",
        "Positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Will $w_1$ and $w_2$ increase or decrease?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Signs of gradients:\n",
        "$$\n",
        "\\frac{\\partial L_{CE}}{\\partial w_1} = (0.5)(3) > 0,\\quad\n",
        "\\frac{\\partial L_{CE}}{\\partial w_2} = (0.5)(2) > 0\n",
        "$$\n",
        "Subtracting positive gradients decreases both weights:\n",
        "- $w_1$ decreases\n",
        "- $w_2$ decreases\n",
        "\n",
        "(Optional numeric update:)\n",
        "$$\n",
        "w_1 \\leftarrow 0 - 0.1(1.5)= -0.15,\\quad\n",
        "w_2 \\leftarrow 0 - 0.1(1.0)= -0.10.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Will $z$ increase or decrease? Will $\\hat{y}$ move toward 1 or toward 0?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With positive features and smaller (more negative) weights, $z=w\\cdot x+b$ decreases. Therefore $\\hat{y}=s(z)$ decreases and moves toward 0.\n",
        "\n",
        "(Optional numeric check:)\n",
        "$$\n",
        "z_{\\text{new}} = (-0.15)(3) + (-0.10)(2) + 0 = -0.65 \\Rightarrow \\hat{y}_{\\text{new}} = s(-0.65) < 0.5.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion (one sentence each)\n",
        "\n",
        "8. In one sentence: explain why $(\\hat{y}-y)$ makes sense as an \"error signal.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$(\\hat{y}-y)$ is an \"error signal\" because its sign encodes whether the model is overpredicting or underpredicting:\n",
        "- if $\\hat{y}>y$, then $\\hat{y}-y>0$ and the update pushes $z$ down\n",
        "- if $\\hat{y}<y$, then $\\hat{y}-y<0$ and the update pushes $z$ up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "9. In one sentence: explain why multiplying by $x_j$ makes sense (why a feature that appears more should change its weight more)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multiplying by $x_j$ makes sense because if feature $j$ appears more in the document (larger $x_j$), it contributed more to the score $z$ for this example, so the update for its weight $w_j$ should be larger in magnitude."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now go back to the whole class and discuss group answers for Part 2 in a plenary session."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
